{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning a constant learning rate is difficult, using an adaptive learning rate is often better. A small learning rate will converge after a very long training process and a large learning rate will reduce the loss quickly but can bounce around the optimum. An adaptive learning rate will change over the course of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential learning rates are a function of the iteration number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ t: \\eta(t) = \\eta_0 10^{\\frac{-t}{r}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires tuning $\\eta_0$ and $r$. At every step, the learning rate will drop by a factor of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper: http://ieeexplore.ieee.org/document/6638963/, the authors compare performance of learning schedules (ways of setting the learning rate). They find that exponential scheduling is simple to implement and converges quickly, compared to others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000 #drop after 10000 steps t\n",
    "decay_rate = 1/10 # drop learning rate by factor of 10 \n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "optimiser = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "training_op = optimiser.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.train.exponential_decay function returns the decayed_learning_rate as:  \n",
    "  \n",
    "learning_rate * decay_rate ^ (global_step / decay_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
