{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through the tutorial available at: http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-04 *\n",
      " -2.4954  0.0000 -2.4951\n",
      "  0.0000 -2.4952  0.0000\n",
      " -2.4952  0.0000 -2.4952\n",
      "  0.0000  0.0000  0.0000\n",
      " -0.1522  0.0000  8.4913\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3) # uninitialised matrix\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4607  0.8523  0.3392\n",
      " 0.6836  0.8309  0.3948\n",
      " 0.3958  0.3686  0.0884\n",
      " 0.9978  0.9993  0.1058\n",
      " 0.7097  0.9369  0.2855\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3) # randomly initialised matrix\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() # returns an object that supports all tuple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9680  1.4975  0.5266\n",
       " 1.0555  0.8871  0.7507\n",
       " 1.3617  0.9848  0.0913\n",
       " 1.1082  1.0515  1.0949\n",
       " 1.5429  0.9503  0.7589\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9680  1.4975  0.5266\n",
       " 1.0555  0.8871  0.7507\n",
       " 1.3617  0.9848  0.0913\n",
       " 1.1082  1.0515  1.0949\n",
       " 1.5429  0.9503  0.7589\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9680  1.4975  0.5266\n",
       " 1.0555  0.8871  0.7507\n",
       " 1.3617  0.9848  0.0913\n",
       " 1.1082  1.0515  1.0949\n",
       " 1.5429  0.9503  0.7589\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result) # output directly into another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9680  1.4975  0.5266\n",
       " 1.0555  0.8871  0.7507\n",
       " 1.3617  0.9848  0.0913\n",
       " 1.1082  1.0515  1.0949\n",
       " 1.5429  0.9503  0.7589\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x) # inplace addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9680  1.4975  0.5266\n",
       " 1.0555  0.8871  0.7507\n",
       " 1.3617  0.9848  0.0913\n",
       " 1.1082  1.0515  1.0949\n",
       " 1.5429  0.9503  0.7589\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operation that mutates a tensor inplace is posted with an _ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch tensors come with all the standard numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8871132135391235"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8871  0.7507\n",
       " 0.9848  0.0913\n",
       " 1.0515  1.0949\n",
       " 0.9503  0.7589\n",
       "[torch.FloatTensor of size 4x2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16) # view is the reshaping operation\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor operation docs: http://pytorch.org/docs/master/torch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy-Torch Bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuda tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be moved onto GPU using the .cuda method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7733  0.0530  1.0625\n",
       " 1.9898  0.4817  1.3803\n",
       " 0.6299  1.2278  1.1665\n",
       " 1.2909  0.3667  0.5617\n",
       " 1.7041  0.8302  0.9406\n",
       "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd.Variable is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call .backward() and have all the gradients computed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the raw tensor through the .data attribute, while the gradient w.r.t. this variable is accumulated into .grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s one more class which is very important for autograd implementation - a Function.\n",
    "\n",
    "Variable and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each variable has a .grad_fn attribute that references a Function that has created the Variable (except for Variables created by the user - their grad_fn is None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x232b988bb38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.grad_fn # returns None because defined by user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out.backward() is equivalent to doing out.backward(torch.Tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.5000  4.5000\n",
       " 4.5000  4.5000\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function and Variable documentation: http://pytorch.org/docs/master/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be constructed using the torch.nn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input)that returns the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a convnet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " (0 ,0 ,.,.) = \n",
       "  -0.0783 -0.1181  0.0529  0.1727 -0.0359\n",
       "   0.1588 -0.0661  0.1984  0.1448 -0.1018\n",
       "   0.0077  0.1965 -0.0743  0.1693  0.0878\n",
       "  -0.0440  0.0827  0.0833  0.1307  0.1294\n",
       "   0.0388 -0.0748  0.0360  0.0542  0.1102\n",
       " \n",
       " (1 ,0 ,.,.) = \n",
       "  -0.0529  0.0670 -0.1865  0.0160  0.1115\n",
       "  -0.0064  0.1919  0.0058  0.1634 -0.1577\n",
       "  -0.1486  0.0105  0.1566 -0.0309 -0.1847\n",
       "  -0.1655  0.1145  0.0934 -0.1488  0.0590\n",
       "  -0.0228  0.0251 -0.0228 -0.0406  0.1407\n",
       " \n",
       " (2 ,0 ,.,.) = \n",
       "   0.0794 -0.1414 -0.0676 -0.0095  0.0038\n",
       "  -0.0505 -0.0623 -0.1938 -0.1157  0.0462\n",
       "  -0.0177  0.0431 -0.1573 -0.1721  0.0525\n",
       "  -0.0614  0.1292  0.0296 -0.1541 -0.1539\n",
       "  -0.0400 -0.0323 -0.0706 -0.0742  0.1712\n",
       " \n",
       " (3 ,0 ,.,.) = \n",
       "   0.0782 -0.1220  0.1012  0.0810  0.1773\n",
       "   0.1694  0.0641 -0.0329 -0.1339 -0.1371\n",
       "  -0.0062 -0.1343  0.0608 -0.1506  0.0833\n",
       "   0.0882  0.0891 -0.0692  0.1241  0.0836\n",
       "  -0.0858 -0.0642 -0.0141  0.1131 -0.0705\n",
       " \n",
       " (4 ,0 ,.,.) = \n",
       "  -0.0152 -0.0155 -0.0272 -0.1815 -0.0815\n",
       "  -0.0014  0.0508  0.1008 -0.0742 -0.0749\n",
       "   0.1788  0.1448  0.0040  0.0022 -0.0949\n",
       "   0.1932 -0.1892 -0.0382 -0.0199  0.1012\n",
       "   0.1109  0.1121  0.0411  0.0816 -0.0796\n",
       " \n",
       " (5 ,0 ,.,.) = \n",
       "  -0.1112 -0.0840 -0.1112  0.1040  0.1522\n",
       "  -0.1161  0.0500  0.1734 -0.1412  0.1914\n",
       "  -0.1568  0.0496  0.0164 -0.0713  0.0133\n",
       "   0.1464  0.1943 -0.1117  0.0486  0.1963\n",
       "   0.0651  0.1236  0.0226  0.1656 -0.0650\n",
       " [torch.FloatTensor of size 6x1x5x5], Parameter containing:\n",
       " -0.1767\n",
       "  0.1851\n",
       "  0.1221\n",
       "  0.0294\n",
       "  0.0825\n",
       "  0.1260\n",
       " [torch.FloatTensor of size 6], Parameter containing:\n",
       " (0 ,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -5.5898 -1.2185 -7.4758  3.7966 -5.0043\n",
       "  -6.2009 -1.7884 -6.5234 -4.2052  7.7536\n",
       "   7.0215  6.9744  1.4298 -6.9870  6.9251\n",
       "   6.1120  5.0787  6.8033 -0.8647 -4.4005\n",
       "  -2.8146  5.9727 -0.5323 -6.2079 -4.8665\n",
       " \n",
       " (0 ,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -8.0838  7.6107 -3.5936 -8.0865  6.1646\n",
       "   3.0985 -5.2953 -3.7643  5.0433  4.8762\n",
       "  -1.5906  6.2195 -4.8951  1.2614 -6.5101\n",
       "  -3.9268 -4.9139  7.5144 -4.7259 -7.5073\n",
       "   3.9568 -1.8045 -7.6469 -6.9562  1.8295\n",
       " \n",
       " (0 ,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -2.5504  3.4247 -1.3598 -4.8947  6.0056\n",
       "  -4.7743 -3.5036  4.6505 -1.2829  7.8321\n",
       "   0.1977 -1.8237  4.7340 -3.1512 -3.8134\n",
       "  -7.9759  2.1240 -0.6009 -0.3020 -0.1650\n",
       "  -3.4184  7.3299  2.5072 -0.9900  1.3014\n",
       " \n",
       " (0 ,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -3.5589  8.0855  0.9215 -4.7470  5.5248\n",
       "  -6.7422 -2.6860  2.7110  4.9018 -0.9804\n",
       "   2.7065 -1.4131  4.5001 -4.3706 -1.7897\n",
       "  -0.9975 -7.4897 -8.0407 -0.9400  2.8188\n",
       "  -0.1480 -5.1166  8.1079  6.7732 -3.9679\n",
       " \n",
       " (0 ,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -2.2074  0.8662  4.1585  5.6550 -0.3099\n",
       "  -4.9902 -7.2988  7.7230  7.4401 -0.4295\n",
       "  -7.6166  2.1674 -4.6347 -7.1430 -4.3350\n",
       "  -5.2027 -6.6640  3.8231  4.9514 -7.2184\n",
       "   1.2222 -6.7074 -2.9844 -5.5398  4.9038\n",
       " \n",
       " (0 ,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   3.4049  0.8685 -0.9276 -6.2291  6.9809\n",
       "   1.9722 -6.6267  3.3878  7.8900 -7.0702\n",
       "   4.0612 -7.3503  7.8270 -2.7161  2.8391\n",
       "  -4.8780  5.8504  4.4644 -5.4537  2.9926\n",
       "   7.4320  5.2761 -5.0832 -0.3570  7.7100\n",
       "      ⋮ \n",
       " \n",
       " (1 ,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -6.4269 -2.8608 -0.6611 -0.9356 -5.6774\n",
       "  -4.9858  4.9381  5.9095 -3.6011  5.3345\n",
       "   7.1075 -7.4467 -0.4818  7.2868  2.0065\n",
       "   7.8420  6.5678 -3.4237  1.2726  5.3541\n",
       "  -1.8180  0.7338 -2.1250  4.5327 -4.4137\n",
       " \n",
       " (1 ,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -4.0799  3.8600  6.5941 -5.6681 -4.8696\n",
       "  -7.1558  4.7969 -3.5271  2.3821  0.9686\n",
       "   2.4274 -2.4996 -1.3812 -6.1493 -5.9743\n",
       "  -3.6971  6.7736  0.4969  1.0612 -5.2727\n",
       "   3.5190 -4.2650 -7.6545 -2.5037 -7.3397\n",
       " \n",
       " (1 ,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   7.6549 -2.4281 -3.7115  1.5527  7.1087\n",
       "   0.3856 -4.4679  3.7694  3.1144 -7.6895\n",
       "   5.8042  2.1933 -0.1166  1.9595 -0.2874\n",
       "  -5.0186 -2.1166 -5.0805  5.1890  5.5129\n",
       "  -4.8582 -6.5066 -3.5041  3.9105 -6.4718\n",
       " \n",
       " (1 ,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   0.2961 -6.4473 -4.6844 -2.0578  2.1896\n",
       "   4.9628  5.2038 -5.2773  0.4364  2.5841\n",
       "   1.4965  0.8502  3.4741  4.5924 -4.8887\n",
       "  -7.4128 -4.0802  2.6095  6.3317  0.8601\n",
       "  -8.1176 -6.9815  4.4365 -5.3600  5.7788\n",
       " \n",
       " (1 ,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -8.0662 -7.6278  2.2857 -3.3150 -7.0645\n",
       "   3.4153  0.6713  6.0272 -2.5816  2.5221\n",
       "  -6.1535  5.5807 -8.0160  3.4971  4.0285\n",
       "  -6.4605 -0.2969  1.8764 -4.8627  4.0298\n",
       "   2.4545  7.0770  0.5843 -3.4557  0.4341\n",
       " \n",
       " (1 ,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -2.6105 -7.1470 -5.2328  1.6698 -6.8634\n",
       "  -5.8350 -6.2807  3.8709 -0.7622  7.8304\n",
       "  -0.3409  0.9694 -0.0378  3.8246 -8.0431\n",
       "  -6.8439  4.3232 -5.4168  0.1824 -5.2864\n",
       "   0.4979  4.5415  8.0161  2.8891  1.3140\n",
       "      ⋮ \n",
       " \n",
       " (2 ,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   3.6672  0.2739 -4.3908  1.0363  6.0249\n",
       "   5.1550 -3.1994 -1.7877  7.9873 -3.8960\n",
       "   7.3941 -3.2452 -0.7185  7.7100  4.8574\n",
       "   6.2782  4.9424 -1.5916 -1.2666  1.0964\n",
       "  -7.4471  8.0494  7.1688 -2.0222  0.1137\n",
       " \n",
       " (2 ,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   6.0674 -8.0421 -0.7588  0.6924 -3.6361\n",
       "  -5.5697 -0.9920  4.7681 -0.8475  4.8414\n",
       "  -4.2080  3.0430  2.5115 -7.1520 -3.5656\n",
       "   5.7095  5.6678  4.1725 -5.9852  8.0803\n",
       "   4.6922 -3.0285 -4.1743  4.0923  2.6455\n",
       " \n",
       " (2 ,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   4.9024  7.7189 -2.5123  6.7435  2.0285\n",
       "   7.5715 -3.1349  1.6673  3.0311 -3.7570\n",
       "  -7.5777  4.6795 -2.6650 -4.5474 -2.0002\n",
       "  -1.4766 -0.7334  0.0864  4.0458 -1.0366\n",
       "   1.2558  5.5291 -2.2678 -6.4209 -4.2652\n",
       " \n",
       " (2 ,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -7.8866 -3.9679  4.6674  1.2869 -5.2402\n",
       "   3.0556  5.7534 -6.2061 -2.7505  7.3863\n",
       "  -5.0146 -0.8018 -8.0493  7.5895  2.9108\n",
       "   7.2570  7.4099  1.7971 -1.9874 -4.3243\n",
       "   2.9156 -2.8517  6.3264 -7.2103  3.1392\n",
       " \n",
       " (2 ,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -6.0330 -2.7140 -4.5360  1.0801  2.8031\n",
       "   6.8294  3.4269 -4.1694 -7.7638  7.3413\n",
       "   0.4331  5.2733  4.3420 -1.1453 -6.3105\n",
       "   1.6411  0.2502  1.5337  5.4431  3.0161\n",
       "   6.2578  7.9583  7.2815 -4.8302 -2.9149\n",
       " \n",
       " (2 ,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -6.2545  4.9537  3.8662  7.3066 -2.4113\n",
       "   4.8165 -0.8104 -1.3739  1.2098  0.2319\n",
       "  -6.5825  4.3172 -0.8330 -0.0895 -4.6199\n",
       "  -0.5946  4.3683  3.0456  0.0997 -4.9449\n",
       "  -0.2414  5.2513 -1.9323  3.3479  2.6077\n",
       " ...   \n",
       "      ⋮ \n",
       " \n",
       " (13,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   6.3091  1.2823 -7.0015  1.3106  3.2806\n",
       "  -2.7887  3.6286  0.0034 -2.3527 -5.3489\n",
       "  -3.7128  2.3959 -1.3943 -3.0319 -0.2255\n",
       "  -4.1454  0.6208  4.9464  5.6353 -2.6651\n",
       "   2.7285 -5.9133  6.2013  4.6246  4.5716\n",
       " \n",
       " (13,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   3.0247 -3.9470  6.8776  5.7607 -3.9797\n",
       "  -3.5984 -3.0184  4.4215 -1.9455  3.6202\n",
       "  -7.8682  8.0134  3.3933  3.8396 -1.0235\n",
       "   5.6215  3.6515  7.2729  6.5846  6.8117\n",
       "  -3.8738  6.7487 -3.1348 -3.7763  0.8965\n",
       " \n",
       " (13,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -0.7551 -8.0680  1.8631 -5.9497 -3.0202\n",
       "  -4.9708  0.1939  7.0863 -7.3435  0.0555\n",
       "   2.9560  2.2030  6.2979  0.1839  4.2241\n",
       "  -5.7896 -0.9420  7.9306 -4.6731 -7.7680\n",
       "   3.7154 -4.6385  1.1904 -2.7294 -2.2574\n",
       " \n",
       " (13,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -3.8572 -6.9084  5.0155 -5.4377 -4.0939\n",
       "   7.6024  3.3146 -0.7673 -7.2375 -4.9362\n",
       "  -3.1911  4.3428 -7.9237 -6.8247 -6.5897\n",
       "   6.5681 -2.8960 -4.4025 -0.8141 -4.2830\n",
       "   0.5815 -8.1576  2.2456  4.6020  0.9997\n",
       " \n",
       " (13,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -4.9553 -2.3957 -5.8689  1.5822  6.0066\n",
       "  -1.3363 -4.2562  6.2394  3.2969  3.1771\n",
       "  -7.0716 -7.1447 -5.7632 -8.0491  6.0286\n",
       "  -1.0615  7.0544  5.7315  5.8286  6.1813\n",
       "   1.3114  1.4611  5.8942  4.7251 -5.2744\n",
       " \n",
       " (13,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   1.0713  7.6066 -1.3527  0.6052  3.9020\n",
       "   7.6587  2.0196  7.9410  5.6833  3.8118\n",
       "  -5.4314  3.1294  7.4959 -2.8324 -1.3595\n",
       "  -1.9789  1.4962 -6.4273 -8.0005 -1.1933\n",
       "  -1.3201 -1.8655 -7.5626 -1.4885 -7.5952\n",
       "      ⋮ \n",
       " \n",
       " (14,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   2.4208 -6.7273 -3.0640 -5.5397  4.9989\n",
       "  -1.1379 -4.1609  6.1778 -0.9734 -1.0015\n",
       "   6.5912  1.0793 -2.1061 -7.7840 -0.1137\n",
       "   0.6573  2.4680  3.4714  4.0519  3.8314\n",
       "   1.1094 -5.6471 -4.6332  1.5679  7.8121\n",
       " \n",
       " (14,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   2.2529 -5.3589  3.6890  3.8213 -3.5143\n",
       "   3.3651 -4.8059 -2.2490 -0.9268  0.4322\n",
       "   7.8613 -7.9045  4.8250 -0.0114 -3.5022\n",
       "  -7.4047 -0.2708 -2.2333 -6.1598 -1.8478\n",
       "  -5.1805  4.5984 -6.4611  0.5291  7.7964\n",
       " \n",
       " (14,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   7.3987 -2.2508  3.9834 -3.1455 -2.6669\n",
       "  -8.0503 -1.2106  1.7223 -1.5515 -1.5669\n",
       "   1.6269  2.6618  3.8295 -0.3527 -1.4944\n",
       "  -1.1747 -1.0614  6.6349  2.2441 -2.5322\n",
       "   2.2132 -4.9217  7.0794  5.2559 -2.3761\n",
       " \n",
       " (14,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   4.5095 -1.3390 -7.2634 -3.2297 -1.7440\n",
       "   0.4207 -2.1897 -6.2695  6.0663 -3.1717\n",
       "  -5.2923  6.8764 -7.7488  6.4863  3.5113\n",
       "   5.9037  1.7839 -3.2940  3.0209  7.4623\n",
       "   5.4171  7.4588  2.9870 -4.2068  2.1099\n",
       " \n",
       " (14,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   1.2635 -2.0609  2.0351 -2.4444 -4.6781\n",
       "  -4.2552  5.0821 -5.5976 -0.3351 -4.3082\n",
       "   7.6452 -3.6564  1.8744  6.7363  0.2920\n",
       "   5.7049 -6.2888  5.3556 -4.1061 -5.3749\n",
       "  -7.7399 -2.1356  8.0032  6.4628 -5.8161\n",
       " \n",
       " (14,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   3.7770 -3.7267 -4.9261  0.7356  7.9427\n",
       "   0.8495 -1.2376 -6.1829  2.2027  0.4079\n",
       "   1.5550  3.6246 -7.7024  7.0617 -6.6193\n",
       "  -7.9922  0.5712  6.9983 -6.1740 -7.5049\n",
       "  -2.5885 -0.5743  2.8525 -6.1397  3.4449\n",
       "      ⋮ \n",
       " \n",
       " (15,0 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -3.4358  5.3863 -1.4527  6.3206  6.3984\n",
       "  -7.2982 -3.7609 -1.0576 -7.3260  1.5226\n",
       "   3.0950 -1.9129  2.1194 -5.4662  4.3777\n",
       "   4.3369  2.5630 -4.6787 -5.4207  2.5225\n",
       "   7.0230  6.1122  6.5365 -6.9535  5.9720\n",
       " \n",
       " (15,1 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   2.5067  1.2144 -0.4978  3.8608 -7.4198\n",
       "  -6.2870  5.2347 -4.3419 -1.7799  2.1183\n",
       "  -4.9263  1.3144 -7.0504  7.9031  2.8326\n",
       "  -1.8256  4.9090 -1.1347  4.3893 -7.9018\n",
       "  -2.7180 -6.6643 -3.6071  4.3322  3.0275\n",
       " \n",
       " (15,2 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   6.6679  1.2974  0.1549 -6.8556  0.5714\n",
       "  -2.0081  7.3343  6.1837  3.0577  3.5969\n",
       "   0.4923  4.0940  6.6061 -7.9095  0.5099\n",
       "   5.1828 -2.4884 -2.5864  1.2526 -6.3778\n",
       "   2.2232  0.6636 -3.1240  3.6715  5.6521\n",
       " \n",
       " (15,3 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "   2.0606 -5.9310  2.3165 -4.4644  7.1890\n",
       "   6.1593  6.4031  6.1133 -4.0332  2.9433\n",
       "   3.8684 -7.7091 -1.8824 -2.7677 -4.5542\n",
       "  -0.0167 -1.8736  3.8651 -1.1615 -3.8504\n",
       "   4.8716 -5.6397  6.4291  1.9366  6.7033\n",
       " \n",
       " (15,4 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -7.7956 -3.9180 -0.4205 -1.6626  4.3585\n",
       "  -3.5018 -4.5059  4.4457  1.9772 -4.0260\n",
       "  -2.3470  0.8704  0.9141  7.5067  0.4523\n",
       "   7.3779  0.0008 -7.4728  1.1207 -6.4625\n",
       "  -2.1361 -3.4185  6.2985  1.4287 -6.3236\n",
       " \n",
       " (15,5 ,.,.) = \n",
       " 1.00000e-02 *\n",
       "  -3.0839  0.3081 -6.1597  5.3511 -2.2541\n",
       "   5.3736  1.8990  0.5840 -3.1034  4.2665\n",
       "   6.7642  2.8243 -4.7318  4.2252  0.2914\n",
       "   3.2435  3.8460  8.0993 -7.1816  7.0813\n",
       "  -0.7438 -1.5192  4.3964  1.5493 -0.1283\n",
       " [torch.FloatTensor of size 16x6x5x5], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "   1.8314\n",
       "   3.0661\n",
       "   5.9379\n",
       "   4.3716\n",
       "   5.7735\n",
       "  -3.3198\n",
       "  -7.5290\n",
       "   4.6861\n",
       "  -6.8659\n",
       "   4.9074\n",
       "   2.1552\n",
       "  -5.1519\n",
       "  -3.2235\n",
       "  -6.7214\n",
       "   4.9706\n",
       "  -6.1209\n",
       " [torch.FloatTensor of size 16], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  1.8213 -0.5607  2.9639  ...  -0.6192  4.0470  3.5484\n",
       "  4.0733  4.4750 -3.4810  ...  -2.9188  0.5008 -2.9575\n",
       "  4.3987  2.1094  0.1319  ...   2.9191  4.5574  2.3467\n",
       "           ...             ⋱             ...          \n",
       "  2.6649  1.6461  0.2326  ...   4.9918 -1.4514  3.3953\n",
       "  0.9923  2.8227  2.9608  ...   2.5615  4.4435 -1.6887\n",
       "  3.8628 -2.8025 -4.1868  ...   1.9015 -0.1453  2.4088\n",
       " [torch.FloatTensor of size 120x400], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "   0.9796\n",
       "  -0.6324\n",
       "   2.9071\n",
       "   3.7934\n",
       "  -0.5956\n",
       "   2.3222\n",
       "   0.2146\n",
       "  -4.0977\n",
       "  -3.2479\n",
       "  -4.9871\n",
       "  -2.0545\n",
       "   1.6374\n",
       "  -4.1313\n",
       "   1.2997\n",
       "  -1.0455\n",
       "   4.2231\n",
       "  -1.2392\n",
       "  -0.4339\n",
       "  -1.6056\n",
       "  -4.5607\n",
       "  -1.0197\n",
       "   1.2986\n",
       "  -3.6742\n",
       "   0.9403\n",
       "  -3.3804\n",
       "   0.9753\n",
       "   4.2434\n",
       "  -4.4695\n",
       "   0.4576\n",
       "  -0.0237\n",
       "  -4.1571\n",
       "  -4.3519\n",
       "  -1.6580\n",
       "  -0.0030\n",
       "  -3.2998\n",
       "   4.9380\n",
       "  -4.9270\n",
       "  -4.3232\n",
       "  -2.3056\n",
       "  -3.6292\n",
       "   1.7972\n",
       "   4.0896\n",
       "  -1.8383\n",
       "   1.8420\n",
       "  -4.2425\n",
       "  -4.1247\n",
       "  -3.7343\n",
       "   2.6516\n",
       "  -4.5706\n",
       "   0.8516\n",
       "  -1.5681\n",
       "  -0.9095\n",
       "   0.3980\n",
       "  -3.9242\n",
       "   3.5753\n",
       "   2.2874\n",
       "   3.4621\n",
       "  -2.8747\n",
       "   0.7604\n",
       "   2.0982\n",
       "   2.1186\n",
       "   0.8440\n",
       "   2.4242\n",
       "   2.8502\n",
       "  -4.5608\n",
       "   3.5866\n",
       "  -3.0080\n",
       "   2.3111\n",
       "   4.7003\n",
       "   1.0952\n",
       "   4.6438\n",
       "   4.5930\n",
       "  -1.9241\n",
       "  -2.7165\n",
       "   3.0978\n",
       "  -3.6919\n",
       "  -4.6654\n",
       "  -0.9469\n",
       "   4.5307\n",
       "  -2.7138\n",
       "   3.5701\n",
       "  -0.4998\n",
       "   3.3065\n",
       "   4.6833\n",
       "  -4.8536\n",
       "   0.6132\n",
       "   0.5301\n",
       "  -4.2613\n",
       "   4.8316\n",
       "   3.1264\n",
       "   3.9178\n",
       "   4.4431\n",
       "  -1.2597\n",
       "   1.1959\n",
       "   4.4385\n",
       "   3.5687\n",
       "  -2.1598\n",
       "   3.7877\n",
       "  -1.3197\n",
       "  -1.1055\n",
       "   2.5228\n",
       "  -3.8881\n",
       "   1.8727\n",
       "  -1.6601\n",
       "   4.2377\n",
       "   0.3457\n",
       "   1.2250\n",
       "   4.8618\n",
       "   4.5556\n",
       "   3.0085\n",
       "  -4.6852\n",
       "  -2.8141\n",
       "   4.6973\n",
       "   3.8557\n",
       "   4.3676\n",
       "   1.7886\n",
       "   2.4790\n",
       "  -2.7163\n",
       "  -0.8610\n",
       "   3.1611\n",
       " [torch.FloatTensor of size 120], Parameter containing:\n",
       " 1.00000e-02 *\n",
       " -2.4383 -6.8432  8.5767  ...   2.3253 -6.2727 -1.3239\n",
       " -3.1485 -7.6617  6.4133  ...   1.7324 -4.6449 -5.7685\n",
       " -6.3240 -7.6985 -1.4603  ...   6.2825  1.2771  5.0946\n",
       "           ...             ⋱             ...          \n",
       "  1.8673 -9.0732 -8.2054  ...   6.9846 -7.1140 -1.9739\n",
       "  7.5348 -3.2438 -8.1409  ...   4.7270  1.7478  7.3942\n",
       " -4.2234 -3.1811 -8.8424  ...   5.5639  6.6292 -2.9509\n",
       " [torch.FloatTensor of size 84x120], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "   5.8956\n",
       "   9.0345\n",
       "   6.8288\n",
       "  -8.7933\n",
       "   5.3497\n",
       "  -4.8620\n",
       "  -7.4621\n",
       "   3.2020\n",
       "  -1.9649\n",
       "   0.9830\n",
       "   2.9812\n",
       "   8.8370\n",
       "   8.8714\n",
       "   6.3373\n",
       "  -4.6198\n",
       "   4.6216\n",
       "   2.2269\n",
       "  -6.3446\n",
       "  -6.9327\n",
       "  -6.6206\n",
       "  -9.1099\n",
       "  -3.2713\n",
       "   5.5250\n",
       "  -0.7690\n",
       "   2.8960\n",
       "   7.7761\n",
       "  -6.6551\n",
       "  -0.3139\n",
       "   8.9264\n",
       "   1.6794\n",
       "   0.2661\n",
       "  -5.3025\n",
       "  -0.0153\n",
       "   3.6103\n",
       "  -4.7738\n",
       "   3.7900\n",
       "   1.5413\n",
       "   8.3817\n",
       "  -0.3767\n",
       "   1.3026\n",
       "   2.6748\n",
       "   1.1091\n",
       "   9.0331\n",
       "   6.8242\n",
       "   4.6410\n",
       "  -0.8923\n",
       "  -2.3726\n",
       "  -7.6106\n",
       "   2.9659\n",
       "   4.7329\n",
       "  -7.0525\n",
       "  -4.2290\n",
       "   6.1918\n",
       "   4.0474\n",
       "   1.3086\n",
       "   5.7645\n",
       "  -5.8306\n",
       "  -5.3228\n",
       "   5.6207\n",
       "   2.1844\n",
       "   7.0539\n",
       "  -1.5866\n",
       "  -1.7599\n",
       "  -3.6063\n",
       "   7.5319\n",
       "  -8.2792\n",
       "   2.3741\n",
       "  -3.2632\n",
       "  -0.8100\n",
       "   7.6370\n",
       "   6.1275\n",
       "  -6.1625\n",
       "   7.2718\n",
       "   1.7956\n",
       "  -8.7499\n",
       "  -2.1627\n",
       "  -3.7828\n",
       "  -0.4445\n",
       "  -7.3790\n",
       "  -2.3947\n",
       "  -0.9875\n",
       "   0.6878\n",
       "  -4.1750\n",
       "   5.1979\n",
       " [torch.FloatTensor of size 84], Parameter containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       "  0.0484 -0.0752 -0.0029  0.0945  0.0570 -0.1054  0.0357  0.0946  0.0552  0.0055\n",
       "  0.0639  0.0419  0.0563  0.0652  0.0409  0.0835 -0.0527 -0.0485  0.0708 -0.0468\n",
       "  0.0593 -0.0186 -0.1058 -0.0184 -0.0945 -0.0931  0.0526  0.0411  0.0593 -0.0091\n",
       "  0.0937  0.0049  0.0769 -0.0120 -0.0271 -0.0931  0.0485 -0.0445 -0.0409  0.0513\n",
       "  0.0913  0.0794  0.0164  0.0920 -0.1021  0.0538  0.0977  0.0366 -0.0093  0.1089\n",
       "  0.0450 -0.0934  0.0646  0.0703  0.1014  0.0447 -0.0183 -0.0451  0.0055  0.0019\n",
       "  0.0072 -0.0162 -0.0290  0.1054 -0.0119 -0.0803 -0.0353 -0.0196 -0.0622 -0.0259\n",
       "  0.0052  0.0283 -0.1042  0.0224  0.0385  0.0174 -0.0258 -0.0583 -0.0011 -0.0471\n",
       " -0.1006 -0.0672  0.0327 -0.0818  0.0048  0.0189 -0.0423 -0.1064  0.0973  0.0405\n",
       " -0.0045 -0.0248 -0.0953  0.0930  0.0443  0.0625  0.0859  0.0498 -0.0046  0.0423\n",
       " \n",
       " Columns 10 to 19 \n",
       " -0.0540 -0.0226 -0.0380 -0.0785 -0.0046 -0.1047  0.0337 -0.0115 -0.0125 -0.0927\n",
       " -0.1021 -0.0100  0.0564  0.0388  0.0593 -0.0835 -0.0419 -0.0547  0.0426  0.0727\n",
       "  0.0755 -0.0692  0.0753 -0.0797  0.0971 -0.0726 -0.0648  0.0849  0.0145 -0.0611\n",
       " -0.0253 -0.0031  0.0909 -0.0589 -0.0378  0.1012 -0.0656  0.0284  0.0526 -0.0306\n",
       "  0.0487  0.0916  0.0549 -0.0734  0.0076 -0.0823 -0.0583  0.0973 -0.0955 -0.0271\n",
       " -0.0182 -0.0081  0.0176  0.0686  0.0139 -0.0020  0.0859 -0.1024 -0.0979 -0.1016\n",
       "  0.0371 -0.0857  0.0832  0.0690 -0.0577 -0.0257  0.0479  0.0473  0.1016  0.0575\n",
       " -0.0093 -0.0712  0.1009  0.0139  0.0648 -0.0865 -0.0546  0.0706  0.0396  0.0298\n",
       " -0.0088  0.0593  0.0339 -0.0047  0.0069 -0.0935 -0.0195 -0.0214 -0.0937  0.0539\n",
       " -0.0478 -0.0330  0.0994  0.0530  0.0695  0.0151 -0.0782 -0.0642 -0.0046  0.0903\n",
       " \n",
       " Columns 20 to 29 \n",
       " -0.0541 -0.0215  0.1088  0.0848 -0.1076  0.0476  0.0613  0.0800  0.0395  0.0957\n",
       " -0.0292  0.0574  0.0523 -0.0706  0.0060 -0.0065 -0.0793  0.1037 -0.0310  0.0811\n",
       "  0.0960 -0.0518 -0.0395  0.0178  0.0935  0.0837 -0.0438  0.0978 -0.0818 -0.0691\n",
       " -0.0731 -0.0708  0.0922 -0.0929  0.0803 -0.1002  0.0318  0.0614 -0.1071  0.1029\n",
       "  0.0246 -0.0727 -0.0906 -0.0866  0.1082  0.0515  0.0612 -0.0376  0.0904 -0.0096\n",
       "  0.0686 -0.0173  0.0813 -0.0216 -0.0684 -0.0968  0.0788  0.0066 -0.0161  0.0908\n",
       " -0.0838  0.0662  0.0972 -0.0643  0.0681  0.0393  0.0613 -0.0158 -0.0050 -0.0898\n",
       "  0.0815 -0.0457  0.0025  0.0090  0.0253  0.1047  0.1037 -0.0066  0.0880 -0.0944\n",
       "  0.0676  0.0241 -0.0156  0.0220 -0.0214  0.0260 -0.0287  0.0778 -0.0744  0.0457\n",
       "  0.0124  0.0112  0.1031  0.0775  0.0069  0.0671 -0.0674 -0.0977 -0.0460  0.0271\n",
       " \n",
       " Columns 30 to 39 \n",
       " -0.0837  0.0441 -0.0793 -0.0206 -0.0135 -0.0162 -0.0846 -0.0829  0.0496 -0.0413\n",
       " -0.0277 -0.0045  0.0800  0.1079  0.0585 -0.0551  0.0270  0.0952 -0.0471 -0.1047\n",
       " -0.0905 -0.0085 -0.0478 -0.0437  0.0593 -0.0467  0.1041 -0.0977 -0.0301 -0.0628\n",
       "  0.0235  0.0259  0.0723 -0.0608  0.0114 -0.0027  0.0282 -0.0568 -0.0635 -0.0557\n",
       "  0.0429 -0.0591 -0.0255  0.0501  0.0702  0.1009  0.0606 -0.0665 -0.0783 -0.0348\n",
       " -0.0031  0.0727 -0.0527 -0.0971 -0.0821 -0.0666 -0.0346  0.1004  0.0420 -0.0339\n",
       "  0.0676  0.0408  0.0400 -0.0565  0.0321 -0.0912  0.0426 -0.0064  0.0786 -0.0972\n",
       " -0.0944  0.0564 -0.0564  0.0797  0.0061 -0.0557  0.0191 -0.0951 -0.0533 -0.0699\n",
       " -0.0246 -0.0583 -0.0932 -0.1039 -0.0204  0.0188 -0.0212 -0.1056  0.0466  0.0055\n",
       " -0.0833 -0.0032  0.0148 -0.0234  0.0769 -0.0287 -0.0253  0.0752  0.0346  0.0288\n",
       " \n",
       " Columns 40 to 49 \n",
       "  0.0528  0.0161  0.0882 -0.0407 -0.0150  0.1042 -0.1060 -0.0960 -0.0822 -0.0428\n",
       "  0.0202 -0.0431 -0.0428 -0.0859  0.0333  0.0370 -0.0758  0.0384 -0.0364 -0.0832\n",
       " -0.0396  0.0717 -0.0880  0.1079  0.0113 -0.0588  0.0259 -0.0087 -0.0330 -0.1042\n",
       " -0.0579  0.0866 -0.0956  0.0986  0.0848 -0.0064  0.0903  0.0599  0.0624 -0.0445\n",
       "  0.0637  0.0064  0.0922  0.0337  0.0120  0.0377  0.0202  0.0700 -0.0776 -0.0962\n",
       "  0.0938  0.0317 -0.0388  0.0202 -0.0181 -0.0645  0.0900 -0.0485 -0.0293 -0.1063\n",
       " -0.0575  0.0543  0.1028 -0.1058 -0.0681  0.0975  0.0677  0.0887 -0.0323  0.0645\n",
       " -0.0893 -0.0220  0.1078 -0.0861  0.0653 -0.0261  0.0276  0.0930  0.0962  0.0507\n",
       " -0.0045  0.0181  0.0143  0.0115 -0.0586 -0.0650  0.0630 -0.0852  0.0572 -0.0905\n",
       " -0.1084  0.1052  0.0948 -0.0616 -0.0582 -0.0144 -0.0862 -0.0589  0.0563 -0.0840\n",
       " \n",
       " Columns 50 to 59 \n",
       "  0.0800 -0.0128  0.0180 -0.1014  0.0385  0.0036 -0.0314 -0.0361  0.0052  0.0595\n",
       " -0.0537 -0.0960  0.0445  0.0485 -0.0529 -0.0752 -0.0333 -0.0340 -0.0847 -0.0748\n",
       "  0.0198  0.0916  0.0893 -0.0075 -0.0559  0.0121  0.0664  0.0766  0.0991 -0.0011\n",
       "  0.0752  0.0448 -0.0535 -0.0766  0.0213 -0.0723  0.1087  0.0736 -0.0070 -0.0450\n",
       "  0.0212  0.0325 -0.0471  0.0939  0.0269 -0.1055 -0.0567  0.0759 -0.0744  0.0480\n",
       " -0.0170  0.0760 -0.0280 -0.1079  0.0146  0.0140  0.0705 -0.0030  0.0423 -0.0505\n",
       " -0.0898 -0.0332  0.0661  0.0455  0.0456  0.0295  0.0449  0.0641  0.0373 -0.0781\n",
       " -0.0040  0.0421 -0.0926  0.0472 -0.0773 -0.0148 -0.0095 -0.0481 -0.0079 -0.0393\n",
       " -0.0116  0.0796 -0.0194 -0.0543 -0.0596 -0.0619 -0.0449  0.0819  0.0483 -0.0782\n",
       " -0.0533  0.0061 -0.0868  0.0037 -0.1079 -0.0905  0.0049 -0.0951  0.1035  0.0573\n",
       " \n",
       " Columns 60 to 69 \n",
       " -0.0366 -0.0926  0.1003  0.0809 -0.0032 -0.0564  0.0830  0.0411  0.0553  0.0573\n",
       " -0.0012  0.0951 -0.0754  0.0378 -0.0385 -0.0111  0.0553 -0.0893  0.1082  0.0131\n",
       " -0.0632 -0.0045 -0.0918  0.1035 -0.0044 -0.0082  0.0862  0.0371  0.0790  0.1011\n",
       "  0.0497  0.0564 -0.0782  0.0175 -0.0219  0.0431 -0.0021  0.0835 -0.0109  0.0247\n",
       " -0.0932  0.0471 -0.0481  0.0695  0.0935 -0.0600 -0.0293 -0.1046  0.0800 -0.0046\n",
       "  0.0125 -0.0222 -0.0755  0.0890  0.0597 -0.0852 -0.0980 -0.0828 -0.0080 -0.0466\n",
       " -0.0911  0.0644 -0.0456 -0.0676  0.0064 -0.1082 -0.0507 -0.0834 -0.0595  0.0591\n",
       " -0.0839 -0.0161 -0.0824  0.0920 -0.0097  0.0670 -0.0286 -0.0698  0.0682  0.0518\n",
       "  0.0636 -0.0750  0.0698  0.0830 -0.0095  0.0749 -0.0695 -0.0242 -0.0586  0.0981\n",
       " -0.0395 -0.0210  0.0144 -0.0864 -0.0062  0.0332  0.0305  0.1000 -0.0527  0.0424\n",
       " \n",
       " Columns 70 to 79 \n",
       "  0.0803 -0.0164 -0.1012 -0.0592  0.0893  0.0166 -0.0337 -0.0038 -0.1042  0.0812\n",
       "  0.0478 -0.0432  0.0976  0.1078 -0.0613 -0.0807 -0.0069  0.0209 -0.0294  0.0627\n",
       "  0.0781 -0.0568  0.0758  0.0829  0.0776  0.0767 -0.0612 -0.0500  0.1048 -0.0474\n",
       " -0.0267  0.0900 -0.0095  0.1030  0.0949  0.0992  0.1055  0.0678 -0.0034  0.0222\n",
       "  0.0596  0.0354 -0.0918  0.0004 -0.0930  0.0102 -0.0784  0.0025  0.0729 -0.0491\n",
       " -0.0526 -0.0165 -0.0991 -0.0228  0.0514  0.0291 -0.0171  0.1071  0.0395 -0.0508\n",
       "  0.0733  0.0399 -0.0178 -0.0127 -0.0556  0.0690 -0.0630  0.0202 -0.0975  0.0003\n",
       " -0.0860  0.0537  0.0743 -0.0926 -0.0311  0.0333  0.0631  0.0192  0.0053 -0.0992\n",
       "  0.0337  0.0895 -0.1019  0.0921 -0.0800  0.0160  0.0136 -0.0809 -0.0891  0.0029\n",
       " -0.0918 -0.0958  0.0735 -0.0990 -0.0643 -0.0649 -0.0112 -0.0290  0.1000 -0.0259\n",
       " \n",
       " Columns 80 to 83 \n",
       " -0.0339 -0.0410  0.0838  0.0071\n",
       " -0.0270 -0.0897  0.0225 -0.0735\n",
       " -0.0331  0.0037 -0.0123  0.0141\n",
       " -0.0198 -0.0036 -0.0212  0.0407\n",
       "  0.0446 -0.0453 -0.0020 -0.1023\n",
       " -0.0030  0.0828 -0.0361 -0.0309\n",
       " -0.0089 -0.0136  0.0312  0.0796\n",
       " -0.0993 -0.0652 -0.0555  0.0546\n",
       " -0.0300  0.0216  0.0552  0.0639\n",
       "  0.0654  0.0960  0.0966 -0.0854\n",
       " [torch.FloatTensor of size 10x84], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -5.3217\n",
       "   3.3714\n",
       "   9.5692\n",
       "   1.8043\n",
       "   2.3481\n",
       "   6.0157\n",
       "   5.0325\n",
       "  -0.4833\n",
       "  -9.7920\n",
       "  -4.7043\n",
       " [torch.FloatTensor of size 10]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.  \n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -3.6797  2.4595  0.1389  3.6198  7.1146  3.3592  2.9009 -6.2261 -8.4600 -2.8673\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.6271\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward at 0x232b9874ef0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.1002\n",
      "-0.0215\n",
      " 0.0067\n",
      " 0.0306\n",
      " 0.0532\n",
      " 0.0024\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn module documentation: http://pytorch.org/docs/master/nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate) # Basic stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch comes with handy optimisers with implementations of Adam etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
