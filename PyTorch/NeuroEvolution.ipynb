{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from operator import itemgetter\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,)) #normalise pixels using mean and stdev\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #normalise to range -1 to 1\n",
    "                   ])\n",
    "\n",
    "\n",
    "\n",
    "MNIST_train = datasets.MNIST(r'D:\\Data_sets/MNIST', train=True, download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "MNIST_test = datasets.MNIST(r'D:\\Data_sets/MNIST', train=False, download=True,\n",
    "                   transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(MNIST_train, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(MNIST_test,\n",
    "                                          batch_size=1000, \n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LAYER_SPACE = {\n",
    "    'nb_units':{'lb': 128, 'ub':1024, 'mutate': 0.15},\n",
    "    'dropout_rate': {'lb': 0.0, 'ub': 0.7, 'mutate': 0.2},\n",
    "    'activation': {'func': ['linear','tanh','relu','sigmoid','elu'], 'mutate':0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NET_SPACE = {\n",
    "    'nb_layers': {'lb': 1, 'ub': 3, 'mutate': 0.15},\n",
    "    'lr': {'lb': 0.001, 'ub':0.1, 'mutate': 0.15},\n",
    "    'weight_decay': {'lb': 0.00001, 'ub': 0.0004, 'mutate':0.2},\n",
    "    'optimizer': {'func': ['sgd', 'adam', 'adadelta','rmsprop'], 'mutate': 0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Randomise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_value(space):\n",
    "    \"\"\"Returns random value from space.\"\"\"\n",
    "    \n",
    "    val = None\n",
    "    \n",
    "    if 'func' in space: #randomise optimiser or activation function\n",
    "        val = random.sample(space['func'], 1)[0] \n",
    "    \n",
    "    elif isinstance(space['lb'], int): #randomise number of units or layers\n",
    "        val = random.randint(space['lb'], space['ub'])\n",
    "    \n",
    "    else: #randomise percentages, i.e. dropout rates or weight decay\n",
    "        val = random.random() * (space['ub'] - space['lb']) + space['lb']\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def randomize_network(layer_space, net_space): \n",
    "    \"\"\"Returns a randomised neural network\"\"\"\n",
    "    net = {}\n",
    "    \n",
    "    for key in net_space.keys():\n",
    "        net[key] = random_value(net_space[key])\n",
    "        \n",
    "    layers = []\n",
    "    \n",
    "    for i in range(net['nb_layers']):\n",
    "        layer = {}\n",
    "        for key in layer_space.keys():\n",
    "            layer[key] = random_value(layer_space[key])\n",
    "        layers.append(layer)\n",
    "        net['layers'] = layers\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'activation': 'elu',\n",
       "   'dropout_rate': 0.2953273247779448,\n",
       "   'nb_units': 863},\n",
       "  {'activation': 'sigmoid',\n",
       "   'dropout_rate': 0.49018793842203023,\n",
       "   'nb_units': 288}],\n",
       " 'lr': 0.03850809968690585,\n",
       " 'nb_layers': 2,\n",
       " 'optimizer': 'rmsprop',\n",
       " 'weight_decay': 0.0002889649100417271}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomize_network(LAYER_SPACE, NET_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mutate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mutate_net(nnet, layer_space, net_space):\n",
    "    \n",
    "    net = copy.deepcopy(nnet)\n",
    "    \n",
    "    \n",
    "    # mutate optimizer\n",
    "    for k in ['lr', 'weight_decay', 'optimizer']:\n",
    "        if random.random() < net_space[k]['mutate']:\n",
    "            net[k] = random_value(net_space[k])\n",
    "    \n",
    "    \n",
    "    # mutate layers\n",
    "    for layer in net['layers']:\n",
    "        for k in layer_space.keys():\n",
    "            if random.random() < layer_space[k]['mutate']:\n",
    "                layer[k] = random_value(layer_space[k])\n",
    "                \n",
    "                \n",
    "    # mutate number of layers -- 50% add 50% remove\n",
    "    if random.random() < net_space['nb_layers']['mutate']:\n",
    "        if net['nb_layers'] <= net_space['nb_layers']['ub']:\n",
    "            if random.random()< 0.5 and \\\n",
    "            net['nb_layers'] < net_space['nb_layers']['ub']:\n",
    "                layer = {}\n",
    "                for key in layer_space.keys():\n",
    "                    layer[key] = random_value(layer_space[key])\n",
    "                net['layers'].append(layer)      \n",
    "            else:\n",
    "                if net['nb_layers'] > 1:\n",
    "                    net['layers'].pop()\n",
    "\n",
    "                \n",
    "            # value & id update\n",
    "            net['nb_layers'] = len(net['layers'])         \n",
    "            \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Flattens input to vector size (batchsize, 1)\n",
    "    (for use in NetFromBuildInfo).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NetFromBuildInfo(nn.Module):\n",
    "    def __init__(self, build_info):\n",
    "        super(NetFromBuildInfo, self).__init__()\n",
    "        \n",
    "        self.activation_dict = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU()\n",
    "            }\n",
    "\n",
    "        #NETWORK DEFINITION\n",
    "        \n",
    "        previous_units = 28 * 28 #MNIST shape\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('flatten', Flatten())\n",
    "         \n",
    "        for i, layer_info in enumerate(build_info['layers']):\n",
    "            i = str(i)\n",
    "            \n",
    "            self.model.add_module(\n",
    "                'fc_' + i,\n",
    "                nn.Linear(previous_units, layer_info['nb_units'])\n",
    "                )\n",
    "            \n",
    "            previous_units = layer_info['nb_units']\n",
    "            \n",
    "            self.model.add_module(\n",
    "                'dropout_' + i,\n",
    "                nn.Dropout(p=layer_info['dropout_rate'])\n",
    "                )\n",
    "            if layer_info['activation'] == 'linear':\n",
    "                continue #linear activation is identity function\n",
    "            self.model.add_module(\n",
    "                layer_info['activation']+ i,\n",
    "                self.activation_dict[layer_info['activation']])\n",
    "\n",
    "        self.model.add_module(\n",
    "            'logits',\n",
    "            nn.Linear(previous_units, 10) #10 MNIST classes\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ##OPTIMIZER\n",
    "\n",
    "        self.opt_args = {#'params': self.model.parameters(),\n",
    "                 'weight_decay': build_info['weight_decay'],\n",
    "                 'lr': build_info['lr']\n",
    "                 }\n",
    "        \n",
    "        self.optimizer_dict = {\n",
    "            'adam': optim.Adam(self.model.parameters(),**self.opt_args),\n",
    "            'rmsprop': optim.RMSprop(self.model.parameters(),**self.opt_args),\n",
    "            'adadelta':optim.Adadelta(self.model.parameters(),**self.opt_args),\n",
    "            'sgd': optim.SGD(self.model.parameters(), **self.opt_args, momentum=0.9) #momentum to train faster\n",
    "            }\n",
    "\n",
    "        self.optimizer = self.optimizer_dict[build_info['optimizer']]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train test helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss /= len(train_loader.dataset)    \n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch, running_loss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \n",
    "    model.train(False)\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TournamentOptimizer:\n",
    "    \"\"\"Define a tournament play selection process.\"\"\"\n",
    "\n",
    "    def __init__(self, population_sz, layer_space, net_space, init_fn, mutate_fn, builder_fn,\n",
    "                 train_fn, test_fn, dataloader, testloader):\n",
    "        \n",
    "        self.init_fn = init_fn\n",
    "        self.layer_space = layer_space\n",
    "        self.net_space = net_space\n",
    "        self.mutate_fn = mutate_fn\n",
    "        self.builder_fn = builder_fn\n",
    "        self.train = train_fn\n",
    "        self.test = test_fn\n",
    "        self.dataloader = dataloader\n",
    "        self.testloader = testloader\n",
    "        self.population_sz = population_sz\n",
    "        \n",
    "        torch.manual_seed(1);\n",
    "        \n",
    "        self.genomes = [init_fn(self.layer_space, self.net_space) for i in range(population_sz)]   \n",
    "        self.population = [NetFromBuildInfo(i).cuda() for i in self.genomes] #randomize population of nets     \n",
    "        \n",
    "        self.test_results = {} \n",
    "        self.genome_history = {} \n",
    "\n",
    "        self.generation = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Tournament evolution step.\"\"\"\n",
    "\n",
    "        genome_holder = [] \n",
    "        \n",
    "        self.generation += 1\n",
    "        \n",
    "        self.genome_history[self.generation] = self.genomes\n",
    "\n",
    "        self.train_nets()\n",
    "        self.evaluate_nets()\n",
    "\n",
    "        mean = np.mean(self.test_results[self.generation]['correct'])\n",
    "        best = np.max(self.test_results[self.generation]['correct'])\n",
    "        \n",
    "        print('\\nPopulation mean:{} max:{}'.format(mean, best))\n",
    "        \n",
    "        \n",
    "        children = []\n",
    "        n_elite = 2\n",
    "        sorted_pop = np.argsort(self.test_results[self.generation]['correct'])[::-1]\n",
    "        elite = sorted_pop[:n_elite]\n",
    "        \n",
    "\n",
    "\n",
    "        # elites always included in the next population\n",
    "        self.elite = []\n",
    "        print('\\nTop performers:')\n",
    "        for no, i in enumerate(elite):\n",
    "            self.elite.append((self.test_results[self.generation]['correct'][i], \n",
    "                               self.population[i]))    \n",
    "            \n",
    "            genome_holder.append(self.genomes[i])\n",
    "            \n",
    "            print(\"{}: score:{}\".format(no, self.test_results[self.generation]['correct'][i]))   \n",
    "            \n",
    "            children.append(self.population[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "        #https://stackoverflow.com/questions/31933784/tournament-selection-in-genetic-algorithm\n",
    "        p = 0.85 # winner probability \n",
    "        tournament_size = 3\n",
    "        probs = [p*((1-p)**i) for i in range(tournament_size-1)]\n",
    "        probs.append(1-np.sum(probs))\n",
    "        #probs = [0.85, 0.1275, 0.0224]\n",
    "        \n",
    "        while len(children) < self.population_sz:\n",
    "            pop = range(len(self.population))\n",
    "            sel_k = random.sample(pop, k=tournament_size)\n",
    "            fitness_k = list(np.array(self.test_results[self.generation]['correct'])[sel_k])\n",
    "            selected = zip(sel_k, fitness_k)\n",
    "            rank = sorted(selected, key=itemgetter(1), reverse=True)\n",
    "            pick = np.random.choice(tournament_size, size=1, p=probs)[0]\n",
    "            best = rank[pick][0]\n",
    "            genome = self.mutate_fn(self.genomes[best], self.layer_space, self.net_space)\n",
    "            print('mutated: ', best)\n",
    "            \n",
    "            genome_holder.append(genome)\n",
    "            model =  self.builder_fn(genome).cuda()\n",
    "            children.append(model)\n",
    "\n",
    "\n",
    "            \n",
    "        self.population = children\n",
    "        self.genomes = genome_holder\n",
    "\n",
    "        \n",
    "        #<----------- add all new genomes to genome history --------->\n",
    "        \n",
    "    def train_nets(self):\n",
    "        \n",
    "        for i, net in enumerate(self.population):\n",
    "            for epoch in range(1, 2):\n",
    "                \n",
    "                torch.manual_seed(1);\n",
    "                \n",
    "                self.train(net, self.dataloader, net.optimizer, epoch)\n",
    "                print('model {} trained'.format(i))\n",
    "                \n",
    "                fp = r\"D:\\Models\\NeuroEvolution/{}-{}\".format(self.generation, i)\n",
    "                torch.save(net.state_dict(), fp)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def evaluate_nets(self):\n",
    "        \"\"\"evaluate the models.\"\"\"\n",
    "        \n",
    "        losses = []\n",
    "        corrects = []\n",
    "        \n",
    "        self.test_results[self.generation] = {}\n",
    "        \n",
    "        for i in range(len(self.population)):\n",
    "            net = self.population[i]\n",
    "            loss, correct = self.test(net, self.testloader)\n",
    "            \n",
    "            losses.append(loss)\n",
    "            corrects.append(correct)\n",
    "        \n",
    "        self.test_results[self.generation]['losses'] = losses\n",
    "        self.test_results[self.generation]['correct'] = corrects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing = TournamentOptimizer(10, LAYER_SPACE, NET_SPACE, randomize_network, \n",
    "                           mutate_net, NetFromBuildInfo, train, test,\n",
    "                          train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 trained\n",
      "model 1 trained\n",
      "model 2 trained\n",
      "model 3 trained\n",
      "model 4 trained\n",
      "model 5 trained\n",
      "model 6 trained\n",
      "model 7 trained\n",
      "model 8 trained\n",
      "model 9 trained\n",
      "\n",
      "Population mean:5769.5 max:9164\n",
      "\n",
      "Top performers:\n",
      "0: score:9164\n",
      "1: score:9159\n",
      "mutated:  8\n",
      "mutated:  8\n",
      "mutated:  1\n",
      "mutated:  0\n",
      "mutated:  5\n",
      "mutated:  2\n",
      "mutated:  0\n",
      "mutated:  5\n"
     ]
    }
   ],
   "source": [
    "testing.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'correct': [8364, 8686, 9159, 2128, 1071, 8834, 8351, 980, 9164, 958],\n",
       "  'losses': [0.5582512512207031,\n",
       "   0.4881768005371094,\n",
       "   0.38637371520996094,\n",
       "   2.410833740234375,\n",
       "   1627.5807125,\n",
       "   0.44339444885253904,\n",
       "   0.6636488464355469,\n",
       "   nan,\n",
       "   0.2797699432373047,\n",
       "   6.1019923828125]}}"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rebuild_from_save(optimizer, generation, position):\n",
    "    \n",
    "    genome = optimizer.genome_history[generation][position]\n",
    "    \n",
    "    net = NetFromBuildInfo(genome)\n",
    "    \n",
    "    net.load_state_dict(torch.load(r\"D:\\Models\\NeuroEvolution\\{}-{}\".format(generation, position)))\n",
    "    \n",
    "    return net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check(optimizer, test_loader):\n",
    "    \n",
    "    for generation in optimizer.test_results:\n",
    "        print('generation 1: \\n')\n",
    "        for i, result in enumerate(optimizer.test_results[generation]['correct']):\n",
    "            \n",
    "            mod = rebuild_from_save(optimizer, generation, i)\n",
    "            _, rebuild_result = test(mod, test_loader)\n",
    "            \n",
    "            print(\"result = {}, rebuild result = {}\".format(result, rebuild_result))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation 1: \n",
      "\n",
      "result = 8364, rebuild result = 8364\n",
      "result = 8686, rebuild result = 8686\n",
      "result = 9159, rebuild result = 9159\n",
      "result = 2128, rebuild result = 2128\n",
      "result = 1071, rebuild result = 1071\n",
      "result = 8834, rebuild result = 8834\n",
      "result = 8351, rebuild result = 8351\n",
      "result = 980, rebuild result = 980\n",
      "result = 9164, rebuild result = 9164\n",
      "result = 958, rebuild result = 958\n"
     ]
    }
   ],
   "source": [
    "sanity_check(testing, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
