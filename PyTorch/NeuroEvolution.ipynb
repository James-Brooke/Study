{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LAYER_SPACE = {\n",
    "    'nb_units':{'lb': 128, 'ub':1024, 'mutate': 0.15},\n",
    "    'dropout_rate': {'lb': 0.0, 'ub': 0.7, 'mutate': 0.2},\n",
    "    'activation': {'func': ['linear','tanh','relu','sigmoid','elu'], 'mutate':0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NET_SPACE = {\n",
    "    'nb_layers': {'lb': 1, 'ub': 3, 'mutate': 0.15},\n",
    "    'lr': {'lb': 0.001, 'ub':0.1, 'mutate': 0.15},\n",
    "    'weight_decay': {'lb': 0.00001, 'ub': 0.0004, 'mutate':0.2},\n",
    "    'optimizer': {'func': ['sgd', 'adam', 'adadelta','rmsprop'], 'mutate': 0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Randomise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_value(space):\n",
    "    \"\"\"Returns random value from space.\"\"\"\n",
    "    \n",
    "    val = None\n",
    "    \n",
    "    if 'func' in space: #randomise optimiser or activation function\n",
    "        val = random.sample(space['func'], 1)[0] \n",
    "    \n",
    "    elif isinstance(space['lb'], int): #randomise number of units or layers\n",
    "        val = random.randint(space['lb'], space['ub'])\n",
    "    \n",
    "    else: #randomise percentages, i.e. dropout rates or weight decay\n",
    "        val = random.random() * (space['ub'] - space['lb']) + space['lb']\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def randomize_network(layer_space, net_space): \n",
    "    \"\"\"Returns a randomised neural network\"\"\"\n",
    "    net = {}\n",
    "    \n",
    "    for key in net_space.keys():\n",
    "        net[key] = random_value(net_space[key])\n",
    "        \n",
    "    layers = []\n",
    "    \n",
    "    for i in range(net['nb_layers']):\n",
    "        layer = {}\n",
    "        for key in layer_space.keys():\n",
    "            layer[key] = random_value(layer_space[key])\n",
    "        layers.append(layer)\n",
    "        net['layers'] = layers\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'activation': 'tanh',\n",
       "   'dropout_rate': 0.055676525173730444,\n",
       "   'nb_units': 132},\n",
       "  {'activation': 'relu', 'dropout_rate': 0.4407416076783757, 'nb_units': 668},\n",
       "  {'activation': 'elu', 'dropout_rate': 0.3728920305155153, 'nb_units': 982}],\n",
       " 'lr': 0.0854667753745106,\n",
       " 'nb_layers': 3,\n",
       " 'optimizer': 'adam',\n",
       " 'weight_decay': 9.064557628116618e-05}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomize_network(LAYER_SPACE, NET_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mutate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mutate_net(net, layer_space, net_space):\n",
    "    \n",
    "    # mutate optimizer\n",
    "    for k in ['lr', 'weight_decay', 'optimizer']:\n",
    "        if random.random() < net_space[k]['mutate']:\n",
    "            net[k] = random_value(net_space[k])\n",
    "    \n",
    "    \n",
    "    # mutate layers\n",
    "    for layer in net['layers']:\n",
    "        for k in layer_space.keys():\n",
    "            if random.random() < layer_space[k]['mutate']:\n",
    "                layer[k] = random_value(layer_space[k])\n",
    "                \n",
    "                \n",
    "    # mutate number of layers -- 50% add 50% remove\n",
    "    if random.random() < net_space['nb_layers']['mutate']:\n",
    "        if net['nb_layers'] <= net_space['nb_layers']['ub']:\n",
    "            if random.random()< 0.5 and \\\n",
    "            net['nb_layers'] < net_space['nb_layers']['ub']:\n",
    "                layer = {}\n",
    "                for key in layer_space.keys():\n",
    "                    layer[key] = random_value(layer_space[key])\n",
    "                net['layers'].append(layer)      \n",
    "            else:\n",
    "                if net['nb_layers'] > 1:\n",
    "                    net['layers'].pop()\n",
    "\n",
    "                \n",
    "            # value & id update\n",
    "            net['nb_layers'] = len(net['layers'])         \n",
    "            \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"For use in NetFromGenome.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetFromGenome():\n",
    "\n",
    "    def __init__(self, build_info, CUDA=True):\n",
    "        super(NetFromGenome, self).__init__()\n",
    "        \n",
    "        previous_units = 28 * 28 #MNIST\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('flatten', Flatten())\n",
    "        \n",
    "        \n",
    "        for i, layer_info in enumerate(build_info['layers']):\n",
    "            i = str(i)\n",
    "            self.model.add_module(\n",
    "                'fc_' + i,\n",
    "                nn.Linear(previous_units, layer_info['nb_units']['val'])\n",
    "                )\n",
    "            self.model.add_module(\n",
    "                'dropout_' + i,\n",
    "                nn.Dropout(p=layer_info['dropout_rate']['val'])\n",
    "                )\n",
    "            if layer_info['activation']['val'] == 'tanh':\n",
    "                self.model.add_module(\n",
    "                    'tanh_'+i,\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "            if layer_info['activation']['val'] == 'relu':\n",
    "                self.model.add_module(\n",
    "                    'relu_'+i,\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            if layer_info['activation']['val'] == 'sigmoid':\n",
    "                self.model.add_module(\n",
    "                    'sigm_'+i,\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "            if layer_info['activation']['val'] == 'elu':\n",
    "                self.model.add_module(\n",
    "                    'elu_'+i,\n",
    "                    nn.ELU()\n",
    "                )\n",
    "            previous_units = layer_info['nb_units']['val']\n",
    "\n",
    "        self.model.add_module(\n",
    "            'classification_layer',\n",
    "            nn.Linear(previous_units, 10)\n",
    "            )\n",
    "        self.model.add_module('sofmax', nn.LogSoftmax())\n",
    "        self.model.cpu()\n",
    "        \n",
    "        if build_info['optimizer']['val'] == 'adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(),\n",
    "                                lr=build_info['weight_decay']['val'],\n",
    "                                weight_decay=build_info['weight_decay']['val'])\n",
    "\n",
    "        elif build_info['optimizer']['val'] == 'adadelta':\n",
    "            optimizer = optim.Adadelta(self.model.parameters(),\n",
    "                                    lr=build_info['weight_decay']['val'],\n",
    "                                    weight_decay=build_info['weight_decay']['val'])\n",
    "\n",
    "        elif build_info['optimizer']['val'] == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(self.model.parameters(),\n",
    "                                    lr=build_info['weight_decay']['val'],\n",
    "                                    weight_decay=build_info['weight_decay']['val'])\n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model.parameters(),\n",
    "                                lr=build_info['weight_decay']['val'],\n",
    "                                weight_decay=build_info['weight_decay']['val'],\n",
    "                                momentum=0.9)\n",
    "        self.optimizer = optimizer\n",
    "        self.cuda = False\n",
    "        if CUDA:\n",
    "            self.model.cuda()\n",
    "            self.cuda = True"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
