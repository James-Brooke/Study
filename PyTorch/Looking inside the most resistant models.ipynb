{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,)) #normalise pixels using mean and stdev\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #normalise to range -1 to 1\n",
    "                   ])\n",
    "\n",
    "\n",
    "\n",
    "MNIST_train = datasets.MNIST(r'D:\\Data_sets/MNIST', train=True, download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "MNIST_test = datasets.MNIST(r'D:\\Data_sets/MNIST', train=False, download=True,\n",
    "                   transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(MNIST_train, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(MNIST_test,\n",
    "                                          batch_size=1000, \n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ModelBuilders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Flattens input to vector size (batchsize, 1)\n",
    "    (for use in NetFromBuildInfo).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NetFromBuildInfo(nn.Module):\n",
    "    def __init__(self, build_info):\n",
    "        super(NetFromBuildInfo, self).__init__()\n",
    "        \n",
    "        self.activation_dict = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU()\n",
    "            }\n",
    "\n",
    "        #NETWORK DEFINITION\n",
    "        \n",
    "        previous_units = 28 * 28 #MNIST shape\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('flatten', Flatten())\n",
    "         \n",
    "        for i, layer_info in enumerate(build_info['layers']):\n",
    "            i = str(i)\n",
    "            \n",
    "            self.model.add_module(\n",
    "                'fc_' + i,\n",
    "                nn.Linear(previous_units, layer_info['nb_units'])\n",
    "                )\n",
    "            \n",
    "            previous_units = layer_info['nb_units']\n",
    "            \n",
    "            self.model.add_module(\n",
    "                'dropout_' + i,\n",
    "                nn.Dropout(p=layer_info['dropout_rate'])\n",
    "                )\n",
    "            if layer_info['activation'] == 'linear':\n",
    "                continue #linear activation is identity function\n",
    "            self.model.add_module(\n",
    "                layer_info['activation']+ i,\n",
    "                self.activation_dict[layer_info['activation']])\n",
    "\n",
    "        self.model.add_module(\n",
    "            'logits',\n",
    "            nn.Linear(previous_units, 10) #10 MNIST classes\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ##OPTIMIZER\n",
    "\n",
    "        self.opt_args = {#'params': self.model.parameters(),\n",
    "                 'weight_decay': build_info['weight_decay'],\n",
    "                 'lr': build_info['lr']\n",
    "                 }\n",
    "        \n",
    "        self.optimizer_dict = {\n",
    "            'adam': optim.Adam(self.model.parameters(),**self.opt_args),\n",
    "            'rmsprop': optim.RMSprop(self.model.parameters(),**self.opt_args),\n",
    "            'adadelta':optim.Adadelta(self.model.parameters(),**self.opt_args),\n",
    "            'sgd': optim.SGD(self.model.parameters(), **self.opt_args, momentum=0.9) #momentum to train faster\n",
    "            }\n",
    "\n",
    "        self.optimizer = self.optimizer_dict[build_info['optimizer']]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rebuild_from_save(df, generation, position):\n",
    "    \n",
    "    genome = df[df['Generation'] == generation].iloc[position, -2]\n",
    "    \n",
    "    net = NetFromBuildInfo(genome)\n",
    "    \n",
    "    net.load_state_dict(torch.load(r\"D:\\Models\\NeuroEvolution\\{}-{}\".format(generation, position)))\n",
    "    \n",
    "    return net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r\"../data/neuroevolution2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Adversarial</th>\n",
       "      <th>No_layers</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Act_func</th>\n",
       "      <th>Nb_units</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Genome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>68</td>\n",
       "      <td>8811</td>\n",
       "      <td>8181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>68</td>\n",
       "      <td>8997</td>\n",
       "      <td>8312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>68</td>\n",
       "      <td>9099</td>\n",
       "      <td>8295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>68</td>\n",
       "      <td>8755</td>\n",
       "      <td>8274</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00463863</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>68</td>\n",
       "      <td>8933</td>\n",
       "      <td>8223</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>68</td>\n",
       "      <td>980</td>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'sgd', 'weight_decay': 4.7129419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>68</td>\n",
       "      <td>9184</td>\n",
       "      <td>627</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0664317</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adadelta', 'weight_decay': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>68</td>\n",
       "      <td>1108</td>\n",
       "      <td>3091</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>68</td>\n",
       "      <td>9207</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039763</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.212479</td>\n",
       "      <td>{'optimizer': 'adadelta', 'weight_decay': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>68</td>\n",
       "      <td>7937</td>\n",
       "      <td>4600</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1822</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'sgd', 'weight_decay': 4.7129419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>68</td>\n",
       "      <td>8820</td>\n",
       "      <td>8131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>324</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>68</td>\n",
       "      <td>9629</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>relu</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adadelta', 'weight_decay': 4.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>68</td>\n",
       "      <td>8967</td>\n",
       "      <td>8217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.524477...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>68</td>\n",
       "      <td>9066</td>\n",
       "      <td>8235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00470045</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>68</td>\n",
       "      <td>8559</td>\n",
       "      <td>7653</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.147497</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>68</td>\n",
       "      <td>9015</td>\n",
       "      <td>8266</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>68</td>\n",
       "      <td>1047</td>\n",
       "      <td>1072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>relu</td>\n",
       "      <td>564</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 2.067522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>68</td>\n",
       "      <td>9170</td>\n",
       "      <td>501</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062212</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.654093</td>\n",
       "      <td>{'optimizer': 'adadelta', 'weight_decay': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>68</td>\n",
       "      <td>9081</td>\n",
       "      <td>8546</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>68</td>\n",
       "      <td>1090</td>\n",
       "      <td>2207</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>68</td>\n",
       "      <td>7817</td>\n",
       "      <td>3044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0408082</td>\n",
       "      <td>linear</td>\n",
       "      <td>1543</td>\n",
       "      <td>0.051383</td>\n",
       "      <td>{'optimizer': 'sgd', 'weight_decay': 0.0001349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>68</td>\n",
       "      <td>980</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>relu</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 4.712941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>68</td>\n",
       "      <td>9112</td>\n",
       "      <td>8577</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'weight_decay': 4.712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>68</td>\n",
       "      <td>8991</td>\n",
       "      <td>8450</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0646036</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.505282</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>68</td>\n",
       "      <td>980</td>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062212</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.538185</td>\n",
       "      <td>{'optimizer': 'sgd', 'weight_decay': 0.0003247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>68</td>\n",
       "      <td>8814</td>\n",
       "      <td>8160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0274471</td>\n",
       "      <td>linear</td>\n",
       "      <td>1381</td>\n",
       "      <td>0.117239</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 1.430719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>68</td>\n",
       "      <td>8739</td>\n",
       "      <td>8175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>68</td>\n",
       "      <td>892</td>\n",
       "      <td>1375</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>564</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 2.067522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>68</td>\n",
       "      <td>9090</td>\n",
       "      <td>8213</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00470045</td>\n",
       "      <td>linear</td>\n",
       "      <td>1286</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 8.656746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>68</td>\n",
       "      <td>958</td>\n",
       "      <td>2882</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'adam', 'weight_decay': 0.000170...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Generation Clean Adversarial No_layers          Lr Act_func Nb_units  \\\n",
       "2010         68  8811        8181         1    0.078663   linear     1577   \n",
       "2011         68  8997        8312         1    0.078663   linear     1577   \n",
       "2012         68  9099        8295         1    0.078663   linear     1577   \n",
       "2013         68  8755        8274         2  0.00463863   linear     1577   \n",
       "2014         68  8933        8223         1    0.078663   linear     1577   \n",
       "2015         68   980         980         1    0.078663   linear     1577   \n",
       "2016         68  9184         627         1   0.0664317   linear     1577   \n",
       "2017         68  1108        3091         1    0.078663     tanh     1577   \n",
       "2018         68  9207         255         1    0.039763   linear     1577   \n",
       "2019         68  7937        4600         1    0.078663  sigmoid     1822   \n",
       "2020         68  8820        8131         1    0.078663   linear      324   \n",
       "2021         68  9629        1869         1    0.078663     relu     1577   \n",
       "2022         68  8967        8217         1    0.078663   linear     1577   \n",
       "2023         68  9066        8235         1  0.00470045   linear     1577   \n",
       "2024         68  8559        7653         1    0.078663   linear     1849   \n",
       "2025         68  9015        8266         1    0.078663   linear     1849   \n",
       "2026         68  1047        1072         1    0.078663     relu      564   \n",
       "2027         68  9170         501         1    0.062212   linear     1577   \n",
       "2028         68  9081        8546         1    0.078663   linear     1995   \n",
       "2029         68  1090        2207         2    0.078663   linear     1577   \n",
       "2030         68  7817        3044         1   0.0408082   linear     1543   \n",
       "2031         68   980         155         1    0.078663     relu     1577   \n",
       "2032         68  9112        8577         1    0.078663   linear     1577   \n",
       "2033         68  8991        8450         2   0.0646036   linear     1577   \n",
       "2034         68   980         980         1    0.062212   linear     1577   \n",
       "2035         68  8814        8160         1   0.0274471   linear     1381   \n",
       "2036         68  8739        8175         1    0.078663   linear     1577   \n",
       "2037         68   892        1375         1    0.078663  sigmoid      564   \n",
       "2038         68  9090        8213         1  0.00470045   linear     1286   \n",
       "2039         68   958        2882         1    0.078663     tanh     1577   \n",
       "\n",
       "       Dropout                                             Genome  \n",
       "2010  0.673602  {'optimizer': 'adam', 'weight_decay': 0.000168...  \n",
       "2011  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2012  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2013  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2014  0.673602  {'optimizer': 'adam', 'weight_decay': 0.000168...  \n",
       "2015  0.673602  {'optimizer': 'sgd', 'weight_decay': 4.7129419...  \n",
       "2016  0.673602  {'optimizer': 'adadelta', 'weight_decay': 0.00...  \n",
       "2017  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2018  0.212479  {'optimizer': 'adadelta', 'weight_decay': 0.00...  \n",
       "2019  0.673602  {'optimizer': 'sgd', 'weight_decay': 4.7129419...  \n",
       "2020  0.673602  {'optimizer': 'adam', 'weight_decay': 0.000168...  \n",
       "2021  0.673602  {'optimizer': 'adadelta', 'weight_decay': 4.71...  \n",
       "2022    0.2942  {'optimizer': 'adam', 'weight_decay': 4.524477...  \n",
       "2023  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2024  0.147497  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2025  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2026  0.673602  {'optimizer': 'adam', 'weight_decay': 2.067522...  \n",
       "2027  0.654093  {'optimizer': 'adadelta', 'weight_decay': 0.00...  \n",
       "2028  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2029  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2030  0.051383  {'optimizer': 'sgd', 'weight_decay': 0.0001349...  \n",
       "2031  0.673602  {'optimizer': 'adam', 'weight_decay': 4.712941...  \n",
       "2032  0.673602  {'optimizer': 'rmsprop', 'weight_decay': 4.712...  \n",
       "2033  0.505282  {'optimizer': 'adam', 'weight_decay': 0.000181...  \n",
       "2034  0.538185  {'optimizer': 'sgd', 'weight_decay': 0.0003247...  \n",
       "2035  0.117239  {'optimizer': 'adam', 'weight_decay': 1.430719...  \n",
       "2036  0.673602  {'optimizer': 'adam', 'weight_decay': 0.000168...  \n",
       "2037  0.673602  {'optimizer': 'adam', 'weight_decay': 2.067522...  \n",
       "2038  0.673602  {'optimizer': 'adam', 'weight_decay': 8.656746...  \n",
       "2039  0.673602  {'optimizer': 'adam', 'weight_decay': 0.000170...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Generation'] == 68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "popnums = [j for i in range(100) for j in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['popnum'] = popnums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Clean</th>\n",
       "      <th>Adversarial</th>\n",
       "      <th>No_layers</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Act_func</th>\n",
       "      <th>Nb_units</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Genome</th>\n",
       "      <th>popnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>68</td>\n",
       "      <td>9112</td>\n",
       "      <td>8577</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078663</td>\n",
       "      <td>linear</td>\n",
       "      <td>1577</td>\n",
       "      <td>0.673602</td>\n",
       "      <td>{'optimizer': 'rmsprop', 'weight_decay': 4.712...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Generation Clean Adversarial No_layers        Lr Act_func Nb_units  \\\n",
       "2032         68  9112        8577         1  0.078663   linear     1577   \n",
       "\n",
       "       Dropout                                             Genome  popnum  \n",
       "2032  0.673602  {'optimizer': 'rmsprop', 'weight_decay': 4.712...      22  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Adversarial'] == df['Adversarial'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'activation': 'linear',\n",
       "   'dropout_rate': 0.6736020681051939,\n",
       "   'nb_units': 1577}],\n",
       " 'lr': 0.07866303147789494,\n",
       " 'nb_layers': 1,\n",
       " 'optimizer': 'rmsprop',\n",
       " 'weight_decay': 4.712941984142251e-05}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Adversarial'] == df['Adversarial'].max()].iloc[0,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = rebuild_from_save(df, 68, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetFromBuildInfo(\n",
       "  (model): Sequential(\n",
       "    (flatten): Flatten()\n",
       "    (fc_0): Linear(in_features=784, out_features=1577, bias=True)\n",
       "    (dropout_0): Dropout(p=0.6736020681051939)\n",
       "    (logits): Linear(in_features=1577, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253725"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = list(model.parameters())[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(layer1.flatten(), bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1577, 784)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0498383"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1[:,783].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = []\n",
    "for i in range(784):\n",
    "    holder.append(layer1[:, i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(holder);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
